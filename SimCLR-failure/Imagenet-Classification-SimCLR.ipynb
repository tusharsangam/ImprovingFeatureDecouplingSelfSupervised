{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from GetDataLoaders import get_dataloaders, get_short_dataloaders\n",
    "from architectures.AlexNetFeatureModified import AlexNetFeature\n",
    "from architectures.TransferLearningNet import Flatten\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "import time\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# we skip the probs for now\\ngama = 2.0\\nwith open(os.path.join(\"./PUprobs\", \\'prob.dat\\'), \\'r\\') as file_input:\\n    train_prob_str = file_input.readlines()\\n    train_prob = [float(i_prob_str.rstrip(\\'\\n\\')) for i_prob_str in train_prob_str]\\n    print(len(train_prob)/4.0)\\n    train_weight = [1.0 if 0==i%4 else 1-train_prob[i]**gama for i in range(len(train_prob))]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# we skip the probs for now\n",
    "gama = 2.0\n",
    "with open(os.path.join(\"./PUprobs\", 'prob.dat'), 'r') as file_input:\n",
    "    train_prob_str = file_input.readlines()\n",
    "    train_prob = [float(i_prob_str.rstrip('\\n')) for i_prob_str in train_prob_str]\n",
    "    print(len(train_prob)/4.0)\n",
    "    train_weight = [1.0 if 0==i%4 else 1-train_prob[i]**gama for i in range(len(train_prob))]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nChannels=256, num_classes=200, pool_size=6, pool_type='max'):\n",
    "        super(Classifier, self).__init__()\n",
    "        nChannelsAll = nChannels * pool_size * pool_size\n",
    "\n",
    "        layers = []\n",
    "        if pool_type == 'max':\n",
    "            layers.append(nn.AdaptiveMaxPool2d((pool_size, pool_size)))\n",
    "            #layers.append(nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "        elif pool_type == 'avg':\n",
    "            layer.append(nn.AdaptiveAvgPool2d((pool_size, pool_size)))\n",
    "        layers.append(nn.BatchNorm2d(nChannels, affine=False))\n",
    "        layers.append(Flatten())\n",
    "        layers.append(nn.Linear(nChannelsAll, num_classes))\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        self.initilize()\n",
    "    \n",
    "    def forward(self, feat):\n",
    "        return self.classifier(feat)\n",
    "    def initilize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fin = m.in_features\n",
    "                fout = m.out_features\n",
    "                std_val = np.sqrt(2.0/fout)\n",
    "                m.weight.data.normal_(0.0, std_val)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = \"cuda\" if use_cuda else \"cpu\"\n",
    "batch_size = 192\n",
    "lr = 0.1\n",
    "LUT_lr = [(5, 0.01),(25, 0.002),(45, 0.0004),(65,0.00008)]\n",
    "num_epochs = 65\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "nesterov = True\n",
    "num_classes = 200\n",
    "loaders = get_dataloaders('imagenet', batch_size=batch_size, num_workers=2, unsupervised=False, simclr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_net = AlexNetFeature().to(device)\n",
    "#load pretrained weights in feature_net\n",
    "state_dict = torch.load(\"weights/AlexNet_Decoupling_Contrastive_SimCLR_Features.pth\")\n",
    "feature_net.load_state_dict(state_dict['featurenet'])\n",
    "\n",
    "feature_net.eval()\n",
    "for param in feature_net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier_net = Classifier().to(device)\n",
    "classifier_optimizer = optim.SGD(classifier_net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "Networks =   {'classifier':classifier_net}\n",
    "Optimizers = {'classifier':classifier_optimizer}\n",
    "\n",
    "Criterions = {'CE': nn.CrossEntropyLoss()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rates(epoch):\n",
    "    # filter out the networks that are not trainable and that do\n",
    "    # not have a learning rate Look Up Table (LUT_lr) in their optim_params\n",
    "    lr = next((lr for (max_epoch, lr) in LUT_lr if max_epoch>epoch), LUT_lr[-1][1])\n",
    "    for key in Optimizers:\n",
    "        for g in Optimizers[key].param_groups:\n",
    "            prev_lr = g['lr']\n",
    "            if prev_lr != lr:\n",
    "                print(\"Learning Rate Updated from {} to {}\".format(prev_lr, lr))\n",
    "                g['lr'] = lr\n",
    "\n",
    "def train_step(batch, train=True):\n",
    "    data, targets = batch\n",
    "    \n",
    "    if train is True:\n",
    "        for key in Optimizers:\n",
    "            Optimizers[key].zero_grad()\n",
    "    \n",
    "    #to cuda\n",
    "    \n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "   \n",
    "    \n",
    "    #collect features\n",
    "    with torch.no_grad():\n",
    "        features = feature_net(data, ['conv5'])\n",
    "    \n",
    "    if train is False:\n",
    "        with torch.no_grad():\n",
    "            pred = Networks['classifier'](features)\n",
    "            #calculate loss\n",
    "            loss_cls =  Criterions['CE'](pred, targets)\n",
    "            \n",
    "    else:\n",
    "        pred = Networks['classifier'](features)\n",
    "        #calculate loss\n",
    "        loss_cls =  Criterions['CE'](pred, targets)\n",
    "    \n",
    "    if train is True:\n",
    "        loss_cls.backward()\n",
    "        for key in Optimizers:\n",
    "            Optimizers[key].step()\n",
    "    \n",
    "    #calculate classification accuracy\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    pred = pred.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(targets.view_as(pred)).sum().item()\n",
    "   \n",
    "    \n",
    "    return loss_cls.item(), correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(data_loader, epoch, train=True):\n",
    "    mode = \"Train\" if train else \"Valid\"\n",
    "    if train is True:\n",
    "        for key in Networks:\n",
    "            Networks[key].train()\n",
    "    else:\n",
    "        for key in Networks:\n",
    "            Networks[key].eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    \n",
    "    #if train:\n",
    "        #adjust_learning_rates(epoch)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    tqdm_bar = tqdm(data_loader)\n",
    "    total_number = 0\n",
    "    for batch_idx, sample in enumerate(tqdm_bar):\n",
    "        \n",
    "        loss, correct_step = train_step(sample, train=train)\n",
    "        losses.append(loss)\n",
    "        correct += correct_step\n",
    "        total_number += sample[0].size(0)\n",
    "        tqdm_bar.set_description('{} Epoch: {} Loss: {:.6f}, Accuracy: {}/{} [{:.4f}%]'.format(mode, epoch, loss, correct, total_number, 100.0*(correct/total_number)))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time for epoch pass {}\".format(end_time-start_time))\n",
    "    \n",
    "    loss = float(np.mean(losses))\n",
    "    acc =  float(correct / len(data_loader.dataset))\n",
    "    print('{} set: Average loss: {:.4f}, Accuracy:{}/{} ({:.4f}%)\\n'.format(mode, loss, correct, len(data_loader.dataset), 100.0*acc))\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def run_main_loop(loaders, num_epochs):\n",
    "    writer = SummaryWriter('logs/Imagenet-Classification-Without-Finetuning-SimCLR')\n",
    "    save_path = \"weights/AlexNet_Decoupling_Contrastive_SimCLR_Classifier.pth\"\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(\"Performing {}th epoch\".format(epoch))\n",
    "        \n",
    "        train_loss, train_acc = train_validate(loaders['train_loader'], epoch, train=True)\n",
    "        val_loss, val_acc = train_validate(loaders['valid_loader'], epoch, train=False)\n",
    "    \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Valid', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        writer.add_scalar('Accuracy/Valid', val_acc, epoch)\n",
    "    \n",
    "        \n",
    "        if val_acc > best_acc  :\n",
    "            best_acc = val_acc\n",
    "            #save model\n",
    "            states = {\n",
    "                'epoch': epoch + 1,\n",
    "                'best_accuracy': best_acc\n",
    "            }\n",
    "            for key in Networks:\n",
    "                states[key+\"model\"] = Networks[key].state_dict()\n",
    "            for key in Optimizers:\n",
    "                states[key+\"optimizer\"] = Optimizers[key].state_dict()\n",
    "            torch.save(states, save_path)\n",
    "            print('Model Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 32.023663, Accuracy: 3379/80000 [4.2237%]: 100%|██████████| 417/417 [01:24<00:00,  4.94it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 84.49328970909119\n",
      "Train set: Average loss: 31.2732, Accuracy:3379/80000 (4.2237%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 0 Loss: 29.262985, Accuracy: 499/10000 [4.9900%]: 100%|██████████| 53/53 [00:09<00:00,  5.60it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 9.471888303756714\n",
      "Valid set: Average loss: 32.0779, Accuracy:499/10000 (4.9900%)\n",
      "\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 Loss: 31.440731, Accuracy: 4460/80000 [5.5750%]: 100%|██████████| 417/417 [01:24<00:00,  4.92it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 84.84145927429199\n",
      "Train set: Average loss: 30.7765, Accuracy:4460/80000 (5.5750%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 1 Loss: 29.046440, Accuracy: 529/10000 [5.2900%]: 100%|██████████| 53/53 [00:10<00:00,  4.99it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 10.613914489746094\n",
      "Valid set: Average loss: 31.4344, Accuracy:529/10000 (5.2900%)\n",
      "\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 Loss: 30.407669, Accuracy: 4964/80000 [6.2050%]: 100%|██████████| 417/417 [01:24<00:00,  4.94it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 84.33255791664124\n",
      "Train set: Average loss: 30.0125, Accuracy:4964/80000 (6.2050%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 2 Loss: 36.864788, Accuracy: 549/10000 [5.4900%]: 100%|██████████| 53/53 [00:09<00:00,  5.51it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 9.62760853767395\n",
      "Valid set: Average loss: 32.0688, Accuracy:549/10000 (5.4900%)\n",
      "\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 Loss: 28.594248, Accuracy: 5258/80000 [6.5725%]: 100%|██████████| 417/417 [01:25<00:00,  4.90it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 85.07449269294739\n",
      "Train set: Average loss: 29.7037, Accuracy:5258/80000 (6.5725%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 3 Loss: 30.684916, Accuracy: 598/10000 [5.9800%]: 100%|██████████| 53/53 [00:09<00:00,  5.48it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 9.680068731307983\n",
      "Valid set: Average loss: 31.4479, Accuracy:598/10000 (5.9800%)\n",
      "\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 Loss: 31.818388, Accuracy: 5370/80000 [6.7125%]: 100%|██████████| 417/417 [01:24<00:00,  4.92it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 84.72269439697266\n",
      "Train set: Average loss: 29.4432, Accuracy:5370/80000 (6.7125%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 4 Loss: 44.573517, Accuracy: 545/10000 [5.4500%]: 100%|██████████| 53/53 [00:09<00:00,  5.37it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 9.868097305297852\n",
      "Valid set: Average loss: 32.0859, Accuracy:545/10000 (5.4500%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 Loss: 32.028893, Accuracy: 5584/80000 [6.9800%]: 100%|██████████| 417/417 [01:24<00:00,  4.91it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 84.95528554916382\n",
      "Train set: Average loss: 29.1784, Accuracy:5584/80000 (6.9800%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid Epoch: 5 Loss: 40.764416, Accuracy: 569/10000 [5.6900%]: 100%|██████████| 53/53 [00:09<00:00,  5.52it/s]\n",
      "  0%|          | 0/417 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch pass 9.602731227874756\n",
      "Valid set: Average loss: 32.1226, Accuracy:569/10000 (5.6900%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 Loss: 29.776878, Accuracy: 2787/38784 [7.1860%]:  48%|████▊     | 201/417 [00:40<00:42,  5.03it/s]"
     ]
    }
   ],
   "source": [
    "run_main_loop(loaders, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = train_validate(loaders['test_loader'], 1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
